{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_6vjeItZVxtv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\ATUL SINGH\\Desktop\\web\\myenv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from apscheduler.schedulers.background import BackgroundScheduler\n",
        "from apscheduler.triggers.cron import CronTrigger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RUIBHZxsWfoG"
      },
      "outputs": [],
      "source": [
        "headers = {\n",
        "    \"Accept\": \"*/*\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.8\",\n",
        "    \"Origin\": \"https://www.moneycontrol.com\",\n",
        "    \"Referer\": \"https://www.moneycontrol.com/\",\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Mobile Safari/537.36\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_length_and_sign(data1):\n",
        "    length = data1[\"c\"] - data1[\"o\"]\n",
        "    sign = length.apply(lambda x: 0 if x < 0 else 1)\n",
        "    data1[\"Length\"] = length\n",
        "    data1[\"Sign\"] = sign\n",
        "    return data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_columnwise(\n",
        "    df, cols, min_max_scaling=False, standard_scaling=False, log_normalization=False\n",
        "):\n",
        "\n",
        "    for col in cols:\n",
        "        if min_max_scaling:\n",
        "\n",
        "            df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
        "        elif standard_scaling:\n",
        "\n",
        "            df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
        "        elif log_normalization:\n",
        "\n",
        "            df[col] = np.log(df[col])\n",
        "        else:\n",
        "            raise ValueError(\"Please specify a normalization technique.\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_data():\n",
        "    data1 = pd.read_csv(\"training.csv\")\n",
        "    data1.set_index(\"time\", inplace=True)\n",
        "    data1[\"Open_Change\"] = data1[\"o\"].pct_change() * 100\n",
        "    data1[\"High_Change\"] = data1[\"h\"].pct_change() * 100\n",
        "    data1[\"Low_Change\"] = data1[\"l\"].pct_change() * 100\n",
        "    data1[\"Close_Change\"] = data1[\"c\"].pct_change() * 100\n",
        "    data1 = calculate_length_and_sign(data1)\n",
        "    data1.dropna(inplace=True)\n",
        "    df_normalized = normalize_columnwise(\n",
        "        data1,\n",
        "        [\"c\", \"o\", \"h\", \"l\"],\n",
        "        min_max_scaling=True,\n",
        "        standard_scaling=False,\n",
        "        log_normalization=False,\n",
        "    )\n",
        "    df_normalized = normalize_columnwise(\n",
        "        data1,\n",
        "        [\"Open_Change\", \"High_Change\", \"Low_Change\", \"Close_Change\", \"Length\"],\n",
        "        min_max_scaling=False,\n",
        "        standard_scaling=True,\n",
        "        log_normalization=False,\n",
        "    )\n",
        "    df_normalized.to_csv(\"normalized_data.csv\")\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "prepare_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_close():\n",
        "    df_normalized = pd.read_csv(\"normalized_data.csv\")\n",
        "    df_normalized.set_index(\"time\", inplace=True)\n",
        "    df_normalized.drop(\"t\", axis=1, inplace=True)\n",
        "    X_train_close = df_normalized.iloc[: int(len(df_normalized) * 0.85)]\n",
        "    X_val_close = df_normalized.iloc[int(len(df_normalized) * 0.85) : -1]\n",
        "    X_test_close = df_normalized[-1:]\n",
        "    Y_train_close = X_train_close[\"c\"]\n",
        "    Y_val_close = X_val_close[\"c\"]\n",
        "    Y_test_close = X_test_close[\"c\"]\n",
        "    X_train_close.drop(\"c\", axis=1, inplace=True)\n",
        "    X_val_close.drop(\"c\", axis=1, inplace=True)\n",
        "    X_test_close.drop(\"c\", axis=1, inplace=True)\n",
        "\n",
        "    model = Sequential(\n",
        "        [\n",
        "            layers.Input((14, 1)),\n",
        "            # Bidirectional LSTMs for extracting both forward and backward context\n",
        "            layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
        "            layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "            # Standard LSTM for further processing\n",
        "            layers.LSTM(32, dropout=0.5),\n",
        "            # Dense layers for final feature extraction and prediction\n",
        "            layers.Dense(32, activation=\"relu\"),  # ReLU activation for non-linearity\n",
        "            layers.Dense(16, activation=\"relu\"),\n",
        "            layers.Dense(1, activation=\"sigmoid\"),  # Sigmoid for binary output\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=[\"mean_absolute_error\"]\n",
        "    )\n",
        "    ealy_stopping = EarlyStopping(patience=10, monitor=\"val_loss\")\n",
        "\n",
        "    model.fit(\n",
        "        X_train_close,\n",
        "        Y_train_close,\n",
        "        epochs=100,\n",
        "        validation_data=(X_val_close, Y_val_close),\n",
        "        callbacks=[ealy_stopping],\n",
        "    )\n",
        "\n",
        "    # Save the model using pickle\n",
        "    # with open('model_close.pkl', 'wb') as f:\n",
        "    #     pickle.dump(model, f)\n",
        "    model.save(\"model_close.h5\")\n",
        "\n",
        "    y_pred = model.predict(X_test_close)\n",
        "    # print(y_pred)\n",
        "\n",
        "    df = pd.read_csv(\"training.csv\")\n",
        "    df.set_index(\"time\", inplace=True)\n",
        "    original_val_close = (y_pred - df_normalized[\"c\"].min()) / (\n",
        "        df_normalized[\"c\"].max() - df_normalized[\"c\"].min()\n",
        "    ) * (df[\"c\"].max() - df[\"c\"].min()) + df[\"c\"].min()\n",
        "    # print(original_val_close)\n",
        "    return original_val_close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_high():\n",
        "    df_normalized = pd.read_csv(\"normalized_data.csv\")\n",
        "    df_normalized.set_index(\"time\", inplace=True)\n",
        "    df_normalized.drop(\"t\", axis=1, inplace=True)\n",
        "    X_train_high = df_normalized.iloc[: int(len(df_normalized) * 0.85)]\n",
        "    X_val_high = df_normalized.iloc[int(len(df_normalized) * 0.85) : -1]\n",
        "    X_test_high = df_normalized[-1:]\n",
        "    Y_train_high = X_train_high[\"h\"]\n",
        "    Y_val_high = X_val_high[\"h\"]\n",
        "    Y_test_high = X_test_high[\"h\"]\n",
        "    X_train_high.drop(\"h\", axis=1, inplace=True)\n",
        "    X_val_high.drop(\"h\", axis=1, inplace=True)\n",
        "    X_test_high.drop(\"h\", axis=1, inplace=True)\n",
        "\n",
        "    model = Sequential(\n",
        "        [\n",
        "            layers.Input((14, 1)),\n",
        "            # Bidirectional LSTMs for extracting both forward and backward context\n",
        "            layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
        "            layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "            # Standard LSTM for further processing\n",
        "            layers.LSTM(32, dropout=0.5),\n",
        "            # Dense layers for final feature extraction and prediction\n",
        "            layers.Dense(32, activation=\"relu\"),  # ReLU activation for non-linearity\n",
        "            layers.Dense(16, activation=\"relu\"),\n",
        "            layers.Dense(1, activation=\"sigmoid\"),  # Sigmoid for binary output\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=[\"mean_absolute_error\"]\n",
        "    )\n",
        "    ealy_stopping = EarlyStopping(patience=10, monitor=\"val_loss\")\n",
        "\n",
        "    model.fit(\n",
        "        X_train_high,\n",
        "        Y_train_high,\n",
        "        epochs=100,\n",
        "        validation_data=(X_val_high, Y_val_high),\n",
        "        callbacks=[ealy_stopping],\n",
        "    )\n",
        "\n",
        "    # Save the model using pickle\n",
        "    # with open('model_high.pkl', 'wb') as f:\n",
        "    #     pickle.dump(model, f)\n",
        "    model.save(\"model_high.h5\")\n",
        "    y_pred = model.predict(X_test_high)\n",
        "    df = pd.read_csv(\"training.csv\")\n",
        "    df.set_index(\"time\", inplace=True)\n",
        "    original_val_high = (y_pred - df_normalized[\"h\"].min()) / (\n",
        "        df_normalized[\"h\"].max() - df_normalized[\"h\"].min()\n",
        "    ) * (df[\"h\"].max() - df[\"h\"].min()) + df[\"h\"].min()\n",
        "    return original_val_high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_low():\n",
        "    df_normalized = pd.read_csv(\"normalized_data.csv\")\n",
        "    df_normalized.set_index(\"time\", inplace=True)\n",
        "    df_normalized.drop(\"t\", axis=1, inplace=True)\n",
        "    X_train_low = df_normalized.iloc[: int(len(df_normalized) * 0.85)]\n",
        "    X_val_low = df_normalized.iloc[int(len(df_normalized) * 0.85) : -1]\n",
        "    X_test_low = df_normalized[-1:]\n",
        "    Y_train_low = X_train_low[\"l\"]\n",
        "    Y_val_low = X_val_low[\"l\"]\n",
        "    Y_test_low = X_test_low[\"l\"]\n",
        "    X_train_low.drop(\"l\", axis=1, inplace=True)\n",
        "    X_val_low.drop(\"l\", axis=1, inplace=True)\n",
        "    X_test_low.drop(\"l\", axis=1, inplace=True)\n",
        "\n",
        "    model = Sequential(\n",
        "        [\n",
        "            layers.Input((14, 1)),\n",
        "            # Bidirectional LSTMs for extracting both forward and backward context\n",
        "            layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
        "            layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "            # Standard LSTM for further processing\n",
        "            layers.LSTM(32, dropout=0.5),\n",
        "            # Dense layers for final feature extraction and prediction\n",
        "            layers.Dense(32, activation=\"relu\"),  # ReLU activation for non-linearity\n",
        "            layers.Dense(16, activation=\"relu\"),\n",
        "            layers.Dense(1, activation=\"sigmoid\"),  # Sigmoid for binary output\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"mse\", optimizer=Adam(learning_rate=0.001), metrics=[\"mean_absolute_error\"]\n",
        "    )\n",
        "    ealy_stopping = EarlyStopping(patience=10, monitor=\"val_loss\")\n",
        "\n",
        "    model.fit(\n",
        "        X_train_low,\n",
        "        Y_train_low,\n",
        "        epochs=100,\n",
        "        validation_data=(X_val_low, Y_val_low),\n",
        "        callbacks=[ealy_stopping],\n",
        "    )\n",
        "\n",
        "    # Save the model using pickle\n",
        "    print(type(model))\n",
        "    # with open('model_low.pkl', 'wb') as f:\n",
        "    #     pickle.dump(model, f)\n",
        "    model.save(\"model_low.h5\")\n",
        "    y_pred = model.predict(X_test_low)\n",
        "    df = pd.read_csv(\"training.csv\")\n",
        "    df.set_index(\"time\", inplace=True)\n",
        "    original_val_low = (y_pred - df_normalized[\"l\"].min()) / (\n",
        "        df_normalized[\"l\"].max() - df_normalized[\"l\"].min()\n",
        "    ) * (df[\"l\"].max() - df[\"l\"].min()) + df[\"l\"].min()\n",
        "    return original_val_low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def update_csv1(timestamp, original_val_high, original_val_low, original_val_close,open_val):\n",
        "  if os.path.isfile(\"frontend_data.csv\") and os.path.getsize(\"frontend_data.csv\") > 30:\n",
        "   \n",
        "    df = pd.read_csv(\"frontend_data.csv\")\n",
        "    print(df.index[-1])\n",
        "    # df.set_index(\"time\", inplace=True)\n",
        "    if int(df['time'][-1:]) == timestamp:\n",
        "        # print('hi')\n",
        "        # remove one row from df\n",
        "        df.drop(df.index[-1], inplace=True)\n",
        "        # print('removed ')\n",
        "        print(df)\n",
        "        n = pd.DataFrame(\n",
        "            {\n",
        "                \"time\": [timestamp],\n",
        "                \"open\": [open_val],\n",
        "                \"high\": [original_val_high],\n",
        "                \"low\": [original_val_low],\n",
        "                \"close\": [original_val_close],\n",
        "            }\n",
        "        )\n",
        "        print(n)\n",
        "        df = pd.concat([df, n])\n",
        "        df.set_index(\"time\", inplace=True)\n",
        "        # print(df)\n",
        "        df.to_csv(\"frontend_data.csv\")\n",
        "        # print(df)\n",
        "\n",
        "        # df.loc[timestamp] = [1,2,3,4]\n",
        "    else:\n",
        "        n = pd.DataFrame(\n",
        "            {\n",
        "                \"time\": [timestamp],\n",
        "                \"open\": [open_val],\n",
        "                \"high\": [original_val_high],\n",
        "                \"low\": [original_val_low],\n",
        "                \"close\": [original_val_close],\n",
        "            }\n",
        "        )\n",
        "        df = pd.concat([df, n])\n",
        "        df.set_index(\"time\", inplace=True)\n",
        "        df.to_csv(\"frontend_data.csv\")\n",
        "        # print(df)\n",
        "    # print(df.index[-1]==timestamp)\n",
        "\n",
        "\n",
        "  else:\n",
        "    df = {\n",
        "        \"time\": [timestamp],\n",
        "        \"open\": [open_val],\n",
        "        \"high\": [original_val_high],\n",
        "        \"low\": [original_val_low],\n",
        "        \"close\": [original_val_close],\n",
        "    }\n",
        "    df = pd.DataFrame(df)\n",
        "    df.set_index(\"time\", inplace=True)\n",
        "    df.to_csv(\"frontend_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def leftlive():\n",
        "    if os.path.isfile(\"leftlive.csv\"):\n",
        "      df=pd.read_csv('today_data.csv')\n",
        "      df[\"time\"] = df[\"time\"].astype(str)\n",
        "      df[\"time\"] = pd.to_datetime(df[\"time\"]).apply(lambda x: int(x.timestamp()))\n",
        "      df=df.drop(['t','hour','minute','year','day','month'], axis=1)\n",
        "      df['open']=df['o']\n",
        "      df['close']=df['c']\n",
        "      df['high']=df['h']\n",
        "      df['low']=df['l']\n",
        "      df=df.drop(['o','h','l','c'],axis=1)\n",
        "      df.set_index('time',inplace=True)\n",
        "      df.to_csv('leftlive.csv')\n",
        "    else:\n",
        "       df = pd.DataFrame(columns=['time', 'open', 'close', 'high', 'low'])\n",
        "       df.set_index('time', inplace=True)\n",
        "       df.to_csv('leftlive.csv')\n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_csv(data, current_time):\n",
        "    print(type(data))\n",
        "    if os.path.isfile(\"today_data.csv\") and os.path.getsize(\"today_data.csv\") > 100:\n",
        "\n",
        "        df = pd.read_csv(\"today_data.csv\")\n",
        "        df.set_index(\"time\", inplace=True)\n",
        "\n",
        "        timestamp = data.index[-1]\n",
        "        if timestamp == df.index[-1]:\n",
        "            df.loc[timestamp] = data.loc[timestamp]\n",
        "        else:\n",
        "            df.loc[timestamp] = data.loc[timestamp]\n",
        "\n",
        "        df.to_csv(\"today_data.csv\")\n",
        "    else:\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(\"today_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_frontend():\n",
        "  prepare_data()\n",
        "       \n",
        "  df = pd.read_csv(\"normalized_data.csv\")\n",
        "# check type of data in time column\n",
        "  df[\"time\"] = df[\"time\"].astype(str)\n",
        "  df[\"time\"] = pd.to_datetime(df[\"time\"]).apply(lambda x: int(x.timestamp()))\n",
        "  timestamp = df[\"time\"].iloc[-1]\n",
        "\n",
        "  new_data_close = df[-1:]\n",
        "  new_data_high = df[-1:]\n",
        "  new_data_low = df[-1:]\n",
        "\n",
        "# #  drop t anc c from new_data_close\n",
        "  df_low = df.drop([\"t\", \"time\"], axis=1)\n",
        "  df_high = df.drop([\"t\", \"time\"], axis=1)\n",
        "  df_close = df.drop([\"t\", \"time\"], axis=1)\n",
        "  new_data_close.drop([\"t\", \"c\", \"time\"], axis=1, inplace=True)\n",
        "  new_data_high.drop([\"t\", \"h\", \"time\"], axis=1, inplace=True)\n",
        "  new_data_low.drop([\"t\", \"l\", \"time\"], axis=1, inplace=True)\n",
        "# print(new_data_close)\n",
        "# load models\n",
        "  model_close = load_model(\"model_close.h5\")\n",
        "  model_high = load_model(\"model_high.h5\")\n",
        "  model_low = load_model(\"model_low.h5\")\n",
        "# predict\n",
        "  close = model_close.predict(new_data_close)\n",
        "  high = model_high.predict(new_data_high)\n",
        "  low = model_low.predict(new_data_low)\n",
        "  close = close[0][0]\n",
        "  high = high[0][0]\n",
        "  low = low[0][0]\n",
        "# close\n",
        "  # print(close, high, low)\n",
        "  data_pred = pd.read_csv(\"training.csv\")\n",
        "  data_pred.set_index(\"time\", inplace=True)\n",
        "  original_val_low = (low - df_low[\"l\"].min()) / (\n",
        "    df_low[\"l\"].max() - df_low[\"l\"].min()\n",
        ") * (data_pred[\"l\"].max() - data_pred[\"l\"].min()) + data_pred[\"l\"].min()\n",
        "\n",
        "  original_val_close = (close - df_close[\"c\"].min()) / (\n",
        "    df_close[\"c\"].max() - df_close[\"c\"].min()\n",
        ") * (data_pred[\"c\"].max() - data_pred[\"c\"].min()) + data_pred[\"c\"].min()\n",
        "  original_val_high = (high - df_high[\"h\"].min()) / (\n",
        "    df_high[\"h\"].max() - df_high[\"h\"].min()\n",
        ") * (data_pred[\"h\"].max() - data_pred[\"h\"].min()) + data_pred[\"h\"].min()\n",
        "  print(original_val_low, original_val_close, original_val_high)\n",
        "\n",
        "  open = data_pred[\"o\"].iloc[-1]\n",
        "  new_data = {\n",
        "    \"timestamp\": [timestamp],\n",
        "    \"close\": [original_val_close],\n",
        "    \"high\": [original_val_high],\n",
        "    \"low\": [original_val_low],\n",
        "    \"open\": [open],\n",
        "}\n",
        "  new_df = pd.DataFrame(new_data)\n",
        "  print(new_df)\n",
        "  update_csv1(timestamp, original_val_high, original_val_low, original_val_close,open)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prepare_frontend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_csv():\n",
        "    df = pd.read_csv(\"Banknifty_data.csv\")\n",
        "    data = pd.read_csv(\"today_data.csv\")\n",
        "\n",
        "    # merge the two dataframes\n",
        "    df = pd.concat([df, data])\n",
        "    df.set_index(\"time\", inplace=True)\n",
        "\n",
        "    # save it to training.csv\n",
        "    df.to_csv(\"training.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def fetch_data():\n",
        "    current_time = datetime.datetime.now()\n",
        "    today = int(datetime.datetime.now().timestamp())\n",
        "    start_time = int(current_time.timestamp())\n",
        "    countback = 500\n",
        "    url = f\"https://priceapi.moneycontrol.com//techCharts/indianMarket/index/history?symbol=in%3Bnbx&resolution=15&from={start_time}&to={today}&countback={countback}&currencyCode=INR\"\n",
        "\n",
        "    # Fetch the data from moneycontrol\n",
        "    res = requests.get(url=url, headers=headers)\n",
        "    df = pd.DataFrame(res.json())\n",
        "    df[\"time\"] = pd.to_datetime(df[\"t\"], unit=\"s\")\n",
        "    df[\"t\"] = pd.to_datetime(df[\"t\"], unit=\"s\")\n",
        "    df[\"time\"] = df[\"time\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
        "    data = df\n",
        "    data.set_index(\"time\", inplace=True)\n",
        "    data.drop([\"s\", \"v\"], axis=1, inplace=True)\n",
        "    data[\"hour\"] = data[\"t\"].dt.hour\n",
        "    data[\"minute\"] = data[\"t\"].dt.minute\n",
        "    data[\"month\"] = data[\"t\"].dt.month\n",
        "    data[\"day\"] = data[\"t\"].dt.day\n",
        "    data[\"year\"] = data[\"t\"].dt.year\n",
        "    print(data)\n",
        "    \n",
        "    \n",
        "    # yo csv\n",
        "    # data.to_csv('Banknifty_data.csv')\n",
        "    # update_csv(data, current_time)\n",
        "    # create_csv()\n",
        "    # prepare_frontend()\n",
        "    # leftlive()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fetch_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preict_candle():\n",
        "    create_csv()\n",
        "    prepare_data()\n",
        "    predict_close()\n",
        "    predict_high()\n",
        "    predict_low()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_csv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\647218034.py:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  new_data_close.drop([\"t\", \"c\", \"time\"], axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\647218034.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  new_data_high.drop([\"t\", \"h\", \"time\"], axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\647218034.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  new_data_low.drop([\"t\", \"l\", \"time\"], axis=1, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "47607.13717589378 47681.08197126984 47624.5590865016\n",
            "    timestamp         close          high           low     open\n",
            "0  1712037600  47681.081971  47624.559087  47607.137176  47618.2\n",
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\3783923540.py:7: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead\n",
            "  if int(df['time'][-1:]) == timestamp:\n"
          ]
        }
      ],
      "source": [
        "prepare_frontend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "leftlive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\2711691790.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train_close.drop(\"c\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\2711691790.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val_close.drop(\"c\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\2711691790.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test_close.drop(\"c\", axis=1, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 9s 152ms/step - loss: 0.0539 - mean_absolute_error: 0.1986 - val_loss: 0.0414 - val_mean_absolute_error: 0.1876\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0446 - mean_absolute_error: 0.1740 - val_loss: 0.0183 - val_mean_absolute_error: 0.1218\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.0324 - mean_absolute_error: 0.1413 - val_loss: 0.0340 - val_mean_absolute_error: 0.1302\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0188 - mean_absolute_error: 0.1136 - val_loss: 0.0363 - val_mean_absolute_error: 0.1468\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 0.0140 - mean_absolute_error: 0.1000 - val_loss: 0.0375 - val_mean_absolute_error: 0.1521\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0137 - mean_absolute_error: 0.0994 - val_loss: 0.0342 - val_mean_absolute_error: 0.1372\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0117 - mean_absolute_error: 0.0922 - val_loss: 0.0387 - val_mean_absolute_error: 0.1538\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0121 - mean_absolute_error: 0.0925 - val_loss: 0.0346 - val_mean_absolute_error: 0.1403\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0106 - mean_absolute_error: 0.0868 - val_loss: 0.0379 - val_mean_absolute_error: 0.1550\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 36ms/step - loss: 0.0122 - mean_absolute_error: 0.0931 - val_loss: 0.0286 - val_mean_absolute_error: 0.1228\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 36ms/step - loss: 0.0106 - mean_absolute_error: 0.0856 - val_loss: 0.0323 - val_mean_absolute_error: 0.1398\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0102 - mean_absolute_error: 0.0844 - val_loss: 0.0286 - val_mean_absolute_error: 0.1277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ATUL SINGH\\Desktop\\web\\myenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\1707539254.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train_high.drop(\"h\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\1707539254.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val_high.drop(\"h\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\1707539254.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test_high.drop(\"h\", axis=1, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 10s 140ms/step - loss: 0.0640 - mean_absolute_error: 0.2169 - val_loss: 0.0297 - val_mean_absolute_error: 0.1588\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0619 - mean_absolute_error: 0.2126 - val_loss: 0.0281 - val_mean_absolute_error: 0.1536\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0513 - mean_absolute_error: 0.1881 - val_loss: 0.0376 - val_mean_absolute_error: 0.1719\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0290 - mean_absolute_error: 0.1323 - val_loss: 0.0435 - val_mean_absolute_error: 0.1508\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0163 - mean_absolute_error: 0.1054 - val_loss: 0.0411 - val_mean_absolute_error: 0.1524\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0150 - mean_absolute_error: 0.1034 - val_loss: 0.0418 - val_mean_absolute_error: 0.1596\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0140 - mean_absolute_error: 0.0989 - val_loss: 0.0405 - val_mean_absolute_error: 0.1561\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0131 - mean_absolute_error: 0.0953 - val_loss: 0.0398 - val_mean_absolute_error: 0.1531\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0127 - mean_absolute_error: 0.0945 - val_loss: 0.0401 - val_mean_absolute_error: 0.1532\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0125 - mean_absolute_error: 0.0925 - val_loss: 0.0384 - val_mean_absolute_error: 0.1531\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0139 - mean_absolute_error: 0.0985 - val_loss: 0.0379 - val_mean_absolute_error: 0.1492\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0132 - mean_absolute_error: 0.0941 - val_loss: 0.0366 - val_mean_absolute_error: 0.1492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ATUL SINGH\\Desktop\\web\\myenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\3578925205.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train_low.drop(\"l\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\3578925205.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val_low.drop(\"l\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_14260\\3578925205.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test_low.drop(\"l\", axis=1, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 9s 159ms/step - loss: 0.0534 - mean_absolute_error: 0.2003 - val_loss: 0.0303 - val_mean_absolute_error: 0.1599\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 0.0514 - mean_absolute_error: 0.1947 - val_loss: 0.0231 - val_mean_absolute_error: 0.1427\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0406 - mean_absolute_error: 0.1675 - val_loss: 0.0154 - val_mean_absolute_error: 0.1006\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0249 - mean_absolute_error: 0.1286 - val_loss: 0.0231 - val_mean_absolute_error: 0.1170\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0154 - mean_absolute_error: 0.1030 - val_loss: 0.0339 - val_mean_absolute_error: 0.1364\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0128 - mean_absolute_error: 0.0967 - val_loss: 0.0340 - val_mean_absolute_error: 0.1414\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0111 - mean_absolute_error: 0.0895 - val_loss: 0.0359 - val_mean_absolute_error: 0.1464\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0113 - mean_absolute_error: 0.0896 - val_loss: 0.0320 - val_mean_absolute_error: 0.1374\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0122 - mean_absolute_error: 0.0944 - val_loss: 0.0317 - val_mean_absolute_error: 0.1319\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0108 - mean_absolute_error: 0.0864 - val_loss: 0.0315 - val_mean_absolute_error: 0.1357\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0105 - mean_absolute_error: 0.0863 - val_loss: 0.0365 - val_mean_absolute_error: 0.1444\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0103 - mean_absolute_error: 0.0846 - val_loss: 0.0300 - val_mean_absolute_error: 0.1320\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0099 - mean_absolute_error: 0.0836 - val_loss: 0.0311 - val_mean_absolute_error: 0.1369\n",
            "<class 'keras.src.engine.sequential.Sequential'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ATUL SINGH\\Desktop\\web\\myenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ],
      "source": [
        "preict_candle()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def del_today_data():\n",
        "    # delete the today_data.csv file\n",
        "    os.remove(\"today_data.csv\")\n",
        "    os.remove(\"frontend_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Job (id=39b9f1fbe8ae4e6b954cbe55a71f729b name=del_today_data)>"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the first scheduler\n",
        "scheduler1 = BackgroundScheduler()\n",
        "trigger1 = CronTrigger(\n",
        "    day_of_week=\"mon-fri\", hour=\"9-10\", minute=\"16,20,25,30,35,40,45,50,51,55,58\"\n",
        ")\n",
        "scheduler1.add_job(fetch_data, trigger1)\n",
        "\n",
        "scheduler2 = BackgroundScheduler()\n",
        "trigger2 = CronTrigger(\n",
        "    day_of_week=\"mon-fri\",\n",
        "    hour=\"10-15\",\n",
        "    minute=\"1,5,10,15,20,25,30,35,40,41,45,50,55,0\",\n",
        ")\n",
        "scheduler2.add_job(fetch_data, trigger2)\n",
        "\n",
        "scheduler3 = BackgroundScheduler()\n",
        "trigger3 = CronTrigger(day_of_week=\"mon-fri\", hour=\"15-16\", minute=\"0,5,10,15,20,25,30\")\n",
        "scheduler3.add_job(fetch_data, trigger3)\n",
        "\n",
        "scheduler4 = BackgroundScheduler()\n",
        "trigger4 = CronTrigger(day_of_week=\"mon-fri\", hour=\"9-15\", minute=\"7,20,33,48,55\")\n",
        "scheduler4.add_job(preict_candle, trigger4)\n",
        "\n",
        "scheduler5 = BackgroundScheduler()\n",
        "trigger5 = CronTrigger(day_of_week=\"mon-fri\", hour=\"15\", minute=\"1,16,31\")\n",
        "scheduler5.add_job(preict_candle,trigger5)\n",
        "\n",
        "scheduler6 = BackgroundScheduler()\n",
        "trigger6 = CronTrigger(day_of_week=\"mon-fri\", hour=\"15\", minute=\"32\")\n",
        "scheduler6.add_job(del_today_data, trigger6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fetch_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preict_candle()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler2.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler2.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_29632\\2711691790.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train_close.drop(\"c\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_29632\\2711691790.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val_close.drop(\"c\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_29632\\2711691790.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test_close.drop(\"c\", axis=1, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "14/14 [==============================] - 20s 283ms/step - loss: 0.0532 - mean_absolute_error: 0.1989 - val_loss: 0.0340 - val_mean_absolute_error: 0.1674\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 0s 28ms/step - loss: 0.0510 - mean_absolute_error: 0.1935 - val_loss: 0.0219 - val_mean_absolute_error: 0.1383\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 1s 64ms/step - loss: 0.0424 - mean_absolute_error: 0.1688 - val_loss: 0.0141 - val_mean_absolute_error: 0.0892\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 1s 84ms/step - loss: 0.0225 - mean_absolute_error: 0.1206 - val_loss: 0.0308 - val_mean_absolute_error: 0.1364\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 1s 77ms/step - loss: 0.0158 - mean_absolute_error: 0.1054 - val_loss: 0.0357 - val_mean_absolute_error: 0.1389\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 1s 75ms/step - loss: 0.0147 - mean_absolute_error: 0.0987 - val_loss: 0.0302 - val_mean_absolute_error: 0.1221\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 1s 72ms/step - loss: 0.0143 - mean_absolute_error: 0.0996 - val_loss: 0.0323 - val_mean_absolute_error: 0.1287\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 1s 69ms/step - loss: 0.0150 - mean_absolute_error: 0.1004 - val_loss: 0.0306 - val_mean_absolute_error: 0.1243\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 1s 65ms/step - loss: 0.0124 - mean_absolute_error: 0.0902 - val_loss: 0.0323 - val_mean_absolute_error: 0.1321\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 1s 65ms/step - loss: 0.0128 - mean_absolute_error: 0.0922 - val_loss: 0.0284 - val_mean_absolute_error: 0.1280\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 1s 72ms/step - loss: 0.0120 - mean_absolute_error: 0.0881 - val_loss: 0.0258 - val_mean_absolute_error: 0.1193\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 1s 70ms/step - loss: 0.0123 - mean_absolute_error: 0.0904 - val_loss: 0.0236 - val_mean_absolute_error: 0.1138\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 1s 60ms/step - loss: 0.0162 - mean_absolute_error: 0.1039 - val_loss: 0.0247 - val_mean_absolute_error: 0.1116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ATUL SINGH\\Desktop\\web\\myenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_29632\\1707539254.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_train_high.drop(\"h\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_29632\\1707539254.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_val_high.drop(\"h\", axis=1, inplace=True)\n",
            "C:\\Users\\ATUL SINGH\\AppData\\Local\\Temp\\ipykernel_29632\\1707539254.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_test_high.drop(\"h\", axis=1, inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "14/14 [==============================] - 25s 417ms/step - loss: 0.0619 - mean_absolute_error: 0.2131 - val_loss: 0.0364 - val_mean_absolute_error: 0.1756\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 1s 51ms/step - loss: 0.0610 - mean_absolute_error: 0.2105 - val_loss: 0.0265 - val_mean_absolute_error: 0.1511\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 1s 50ms/step - loss: 0.0527 - mean_absolute_error: 0.1921 - val_loss: 0.0202 - val_mean_absolute_error: 0.1111\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0225 - mean_absolute_error: 0.1189 - val_loss: 0.0448 - val_mean_absolute_error: 0.1566\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 1s 65ms/step - loss: 0.0210 - mean_absolute_error: 0.1197 - val_loss: 0.0345 - val_mean_absolute_error: 0.1296\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 1s 83ms/step - loss: 0.0196 - mean_absolute_error: 0.1152 - val_loss: 0.0381 - val_mean_absolute_error: 0.1380\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 1s 67ms/step - loss: 0.0157 - mean_absolute_error: 0.1030 - val_loss: 0.0412 - val_mean_absolute_error: 0.1451\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 1s 70ms/step - loss: 0.0143 - mean_absolute_error: 0.0987 - val_loss: 0.0411 - val_mean_absolute_error: 0.1469\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 1s 64ms/step - loss: 0.0140 - mean_absolute_error: 0.0966 - val_loss: 0.0385 - val_mean_absolute_error: 0.1444\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 1s 66ms/step - loss: 0.0144 - mean_absolute_error: 0.0984 - val_loss: 0.0444 - val_mean_absolute_error: 0.1771\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 1s 62ms/step - loss: 0.0124 - mean_absolute_error: 0.0887 - val_loss: 0.0414 - val_mean_absolute_error: 0.1686\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 3s 190ms/step - loss: 0.0149 - mean_absolute_error: 0.0972 - val_loss: 0.0451 - val_mean_absolute_error: 0.1850\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 0.0118 - mean_absolute_error: 0.0868 - val_loss: 0.0509 - val_mean_absolute_error: 0.2032\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "14/14 [==============================] - 19s 338ms/step - loss: 0.0513 - mean_absolute_error: 0.1937 - val_loss: 0.0304 - val_mean_absolute_error: 0.1583\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 1s 55ms/step - loss: 0.0470 - mean_absolute_error: 0.1840 - val_loss: 0.0198 - val_mean_absolute_error: 0.1263\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 1s 54ms/step - loss: 0.0333 - mean_absolute_error: 0.1436 - val_loss: 0.0299 - val_mean_absolute_error: 0.1298\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 1s 49ms/step - loss: 0.0194 - mean_absolute_error: 0.1117 - val_loss: 0.0338 - val_mean_absolute_error: 0.1329\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 1s 60ms/step - loss: 0.0137 - mean_absolute_error: 0.0984 - val_loss: 0.0361 - val_mean_absolute_error: 0.1383\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 1s 67ms/step - loss: 0.0137 - mean_absolute_error: 0.0966 - val_loss: 0.0330 - val_mean_absolute_error: 0.1314\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 1s 71ms/step - loss: 0.0151 - mean_absolute_error: 0.1013 - val_loss: 0.0337 - val_mean_absolute_error: 0.1328\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 1s 63ms/step - loss: 0.0132 - mean_absolute_error: 0.0952 - val_loss: 0.0352 - val_mean_absolute_error: 0.1413\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 1s 62ms/step - loss: 0.0126 - mean_absolute_error: 0.0940 - val_loss: 0.0285 - val_mean_absolute_error: 0.1301\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 1s 70ms/step - loss: 0.0120 - mean_absolute_error: 0.0882 - val_loss: 0.0373 - val_mean_absolute_error: 0.1540\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 1s 73ms/step - loss: 0.0113 - mean_absolute_error: 0.0876 - val_loss: 0.0291 - val_mean_absolute_error: 0.1398\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 0.0104 - mean_absolute_error: 0.0828 - val_loss: 0.0397 - val_mean_absolute_error: 0.1591\n",
            "<class 'keras.src.engine.sequential.Sequential'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ATUL SINGH\\Desktop\\web\\myenv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        }
      ],
      "source": [
        "scheduler4.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler4.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler1.start()\n",
        "scheduler2.start()\n",
        "scheduler3.start()\n",
        "scheduler4.start()\n",
        "scheduler5.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler1.shutdown()\n",
        "scheduler2.shutdown()\n",
        "scheduler3.shutdown()\n",
        "scheduler4.shutdown()\n",
        "scheduler5.shutdown()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
